{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30610761",
   "metadata": {},
   "source": [
    "# **EXP 1 – Brown & Penn Treebank Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9329314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Categories: ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "Brown Sample: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that']\n",
      "Penn Treebank Sample: ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.', 'Mr.', 'Vinken']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, treebank\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "\n",
    "print(\"Brown Categories:\", brown.categories())\n",
    "print(\"Brown Sample:\", brown.words(categories='news')[:20])\n",
    "print(\"Penn Treebank Sample:\", treebank.words()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af465376",
   "metadata": {},
   "source": [
    "# **EXP 2 – Sentence & Word Segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3db06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      "I love coding.\n",
      "NLP is amazing.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coding. NLP is amazing.\")\n",
    "\n",
    "print(\"Sentences:\")\n",
    "for s in doc.sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ccf46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Word Tokenize: ['Hi', '!', 'Let', \"'s\", 'test', 'segmentation', '.']\n",
      "Regex Tokenize: ['I', 'Love', 'Python']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "text = \"Hi! Let's test segmentation.\"\n",
    "print(\"NLTK Word Tokenize:\", word_tokenize(text))\n",
    "print(\"Regex Tokenize:\", RegexpTokenizer(r'\\s+', gaps=True).tokenize(\"I Love Python\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf3673b",
   "metadata": {},
   "source": [
    "# **EXP 3 – Tokenization Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6814dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank: ['Hello', 'World', '!', 'Let', \"'s\", 'test', 'tokenizers', '.']\n",
      "wordpunct: ['Hello', 'World', '!', 'Let', \"'\", 's', 'test', 'tokenizers', '.']\n",
      "Sentences: ['Hello World!', \"Let's test tokenizers.\"]\n",
      "Whitespace: ['Hello', 'World!', \"Let's\", 'test', 'tokenizers.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer, wordpunct_tokenize, sent_tokenize, WhitespaceTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"Hello World! Let's test tokenizers.\"\n",
    "print(\"Treebank:\", TreebankWordTokenizer().tokenize(text))\n",
    "print(\"wordpunct:\", wordpunct_tokenize(text))\n",
    "print(\"Sentences:\", sent_tokenize(text))\n",
    "print(\"Whitespace:\", WhitespaceTokenizer().tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d740381",
   "metadata": {},
   "source": [
    "# **EXP 4 – Lemmatization & Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad26775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word -> Porter | Lancaster | Snowball | Lemmatizer\n",
      "running -> run run run running\n",
      "flies -> fli fli fli fly\n",
      "wolves -> wolv wolv wolv wolf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "words = [\"running\", \"flies\", \"wolves\"]\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()\n",
    "ss = SnowballStemmer(\"english\")\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "print(\"Word -> Porter | Lancaster | Snowball | Lemmatizer\")\n",
    "for w in words:\n",
    "    print(w, \"->\", ps.stem(w), ls.stem(w), ss.stem(w), lm.lemmatize(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c91c0",
   "metadata": {},
   "source": [
    "# **EXP 5 – Text Normalization & N-Grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70f5a5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['i', 'am', 'learning', 'nlp', 'it', 'is', 'fun', 'is', 'not', 'it']\n",
      "Unigrams: [('i',), ('am',), ('learning',), ('nlp',), ('it',), ('is',), ('fun',), ('is',), ('not',), ('it',)]\n",
      "Bigrams: [('i', 'am'), ('am', 'learning'), ('learning', 'nlp'), ('nlp', 'it'), ('it', 'is'), ('is', 'fun'), ('fun', 'is'), ('is', 'not'), ('not', 'it')]\n",
      "Trigrams: [('i', 'am', 'learning'), ('am', 'learning', 'nlp'), ('learning', 'nlp', 'it'), ('nlp', 'it', 'is'), ('it', 'is', 'fun'), ('is', 'fun', 'is'), ('fun', 'is', 'not'), ('is', 'not', 'it')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"I'm learning NLP!!! It's fun, isn't it?\"\n",
    "text = contractions.fix(text)\n",
    "clean = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "tokens = word_tokenize(clean)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Unigrams:\", list(ngrams(tokens,1)))\n",
    "print(\"Bigrams:\", list(ngrams(tokens,2)))\n",
    "print(\"Trigrams:\", list(ngrams(tokens,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016be74d",
   "metadata": {},
   "source": [
    "# **EXP 6 – POS Tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae631b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(nltk.pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60bcf0",
   "metadata": {},
   "source": [
    "# **EXP 7 – Named Entity Recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c78143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barack Obama -> PERSON\n",
      "Hawaii -> GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Barack Obama was born in Hawaii.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \"->\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f007c",
   "metadata": {},
   "source": [
    "# **EXP 8 – Dependency Parsing & Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6940ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Th3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = pos_tag(word_tokenize(text))\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = RegexpParser(grammar)\n",
    "print(cp.parse(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "734f6626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The -> det -> developer\n",
      "young -> amod -> developer\n",
      "developer -> nsubj -> solved\n",
      "solved -> ROOT -> solved\n",
      "the -> det -> issue\n",
      "issue -> dobj -> solved\n",
      "quickly -> advmod -> solved\n",
      ". -> punct -> solved\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"The young developer solved the issue quickly.\")\n",
    "for t in doc:\n",
    "    print(t.text, \"->\", t.dep_, \"->\", t.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89be73a5",
   "metadata": {},
   "source": [
    "# **EXP 9 – Word Embeddings (Word2Vec & BERT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ced0a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03655883  0.02535131  0.03378846  0.00381433  0.03175445 -0.01702683\n",
      " -0.00473201  0.02884287 -0.03760819 -0.01968052]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec([[\"this\",\"is\",\"word2vec\",\"test\"]], vector_size=20, min_count=1)\n",
    "print(model.wv[\"word2vec\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61aa59c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT CLS Embedding (first 10 values):\n",
      "tensor([-0.0039,  0.3164,  0.0708, -0.3312, -0.6144, -0.4645,  0.1999,  0.8702,\n",
      "         0.0341, -0.3119])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"I love NLP.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "cls_embedding = outputs.last_hidden_state[0][0]\n",
    "\n",
    "print(\"BERT CLS Embedding (first 10 values):\")\n",
    "print(cls_embedding[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f650b7",
   "metadata": {},
   "source": [
    "# **EXP 10 – Sentiment Analysis & Fake News Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2ee0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I really love this NLP practical!\n",
      "Sentiment Polarity: 0.625\n",
      "Sentiment Subjectivity: 0.6\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I really love this NLP practical!\"\n",
    "blob = TextBlob(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Sentiment Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Sentiment Subjectivity:\", blob.sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcf753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "texts = [\"Fake news spreading!\", \"Government released report\"]\n",
    "labels = [1, 0]\n",
    "\n",
    "X = TfidfVectorizer().fit_transform(texts)\n",
    "clf = LogisticRegression().fit(X, labels)\n",
    "print(clf.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd63ea4a",
   "metadata": {},
   "source": [
    "# **EXP 11 – Fine-Tuning HuggingFace Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dea6b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 1057.11 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 871.54 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready. Run trainer.train() to fine-tune.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "ds = load_dataset(\"imdb\", split=\"train[:1%]\").train_test_split(0.2)\n",
    "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def encode(e): \n",
    "    return tok(e[\"text\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "ds = ds.map(encode)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\"out\", per_device_train_batch_size=4, num_train_epochs=1)\n",
    ")\n",
    "\n",
    "print(\"Model ready. Run trainer.train() to fine-tune.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
